# 機械学習ライブラリのデータセット周りの調査

## 1. 対象ライブラリ

+ Keras
+ Chainer
+ Pytorch
+ scikit-learn
+ (Theano)
+ (Caffe)

---

## 2. 各ライブラリのデータセットの形式

### **Keras**

**kerasにおける学習データの扱い**

- 入力データのNumpy配列と対応するラベルデータのNumpy配列をそれぞれfitに渡す
- kerasの形式に従ったジェネレータを作成してfit_generatorに投げる
- Sequenceクラスを継承したデータセットクラスのようなものを作成しfit_generatorに投げる

### **Chainer**

**Chainerにおける学習データの扱い**

- chainer.datasetクラスを継承して作成したデータセットクラスを使って学習

    データセットクラス内でミニバッチも作成している模様

### **Pytorch**

**Pytorchにおける学習データの扱い**

ぶっちゃけPytorchのデータセットの形はなんでもいい(自分でバッチ作ってforwardしてbackwardしてstepすればいいから)

- torchvisionを用いたデータセットクラスを作成し利用
- 入力データとラベルデータをそれぞれ何らかの配列で用意し利用

### **scikit-learn**

基本的に入力データとラベルデータのNumpy配列をそれぞれ用意してfitで学習

ただし，irisデータセットなどのsklearnで用意されている有名データセットはBunchというdictionary-likeなクラスのインスタンスとして提供されている模様
ただ結局このデータセットからNumpy配列に変換するのでBunchはsklearnの学習用のデータセットとは言えないかもしれない．

Bunchは今回の汎用データセットの枠組みを作る上でその実装方法が参考になりそう．

### **Theano**

sklearnと同じ形式だと思われる．

### **Caffe**

sklearnと同じ形式だと思われる．

---

## **3. 扱うデータの種類**

+ 画像データ
+ シーケンスデータ
+ メタデータ


## **4. メモリ不足対策**

**汎用的な対策**

[【Python】Pythonでメモリ不足になったときにすること](https://www.haya-programming.com/entry/2017/02/09/190512#%E3%83%95%E3%82%A1%E3%82%A4%E3%83%AB%E3%81%AB%E3%83%80%E3%83%B3%E3%83%97%E3%81%99%E3%82%8B)

**Pandaを用いた方法**

Pandasにはcsvファイルを一度に全て読み込むのではなく，部分(Chunk)に分けて細かくロードすることもできる．
任意のデータセットを一度csv形式に落とし込んでからpandasで再度読み込む形を取れば，汎用性はやや損なうが，扱いやすさは向上する．
落とし込んだcsvファイルは後でキャッシュとしても利用することができると同時に，一種の中間表現としても利用できる．

## **5. 実装**

**問題点・工夫**

+ 中間表現をどういう形にするか？
+ HASKのようなシーケンスデータをどうするか？
+ 各ライブラリへの変換をどうするか？
+ インターフェースをどうするか？
+

**Note**

**汎用データセットの役割**

+ HASKやImageNetといった有名デーセットをサポートし、車輪の再開発を防ぐこと
+ 独自デーセットも含む多種多様なデータセットをマネジメントし、同じデータセットに対しライブラリごとに実装を行う手間をなくす
+ 生のデータと各ライブラリとの中間的な立ち位置のシステム
+ キャッシュ機能を提供し、2回目以降の実行を高速化する
